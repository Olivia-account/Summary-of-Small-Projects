{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "data = pd.read_csv('test3.csv', encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将\"order_dt\"列转换为日期类型\n",
    "data['order_dt'] = pd.to_datetime(data['order_dt'])\n",
    "\n",
    "# 创建条件来选择在指定日期范围内的数据\n",
    "start_date = pd.to_datetime(\"2022-09-01\")\n",
    "end_date = pd.to_datetime(\"2022-09-30\")\n",
    "mask_train = (data['order_dt'] >= start_date) & (data['order_dt'] <= end_date)\n",
    "mask_test = (~(data['order_dt'] >= start_date) & (data['order_dt'] <= end_date))\n",
    "\n",
    "test1 = data[mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX=[]#属性\n",
    "dataY=[]#标签\n",
    "k=0\n",
    "tempX=[]#储存某个历史167天数据\n",
    "tempY=[]#储存某个未来1天数据,即第168天\n",
    "for index, rows in data.iterrows():\n",
    "    if k<168:\n",
    "        k+=1\n",
    "        tempX.append([rows['pay_num'],rows['payment_orders'],rows['refund_orders'],rows['cancel_orders'],rows['income'],rows['refund_income'],rows['actual_orders'],rows['续费率'],rows['实际收入'],rows['保留率'],rows['ltv']])\n",
    "        \n",
    "        continue\n",
    "    if k<169:\n",
    "        k = 0\n",
    "        # print(rows['week'])\n",
    "        tempY.append([rows['pay_num'],rows['payment_orders'],rows['refund_orders'],rows['cancel_orders'],rows['income'],rows['refund_income'],rows['actual_orders'],rows['续费率'],rows['实际收入'],rows['保留率'],rows['ltv']])\n",
    "        \n",
    "        dataX.append(tempX)\n",
    "        dataY.append(tempY)\n",
    "        tempX=[]#储存某个历史167天数据\n",
    "        tempY=[]#储存某个未来1天数据,即第168天\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    " \n",
    "dataX=torch.tensor(dataX)#列表转Tensor\n",
    "dataY=torch.tensor(dataY)#列表转Tensor\n",
    " \n",
    "dataset=Data.TensorDataset(dataX,dataY)\n",
    "train_size=int(0.8*len(dataset))\n",
    "test_size=len(dataset)-train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])#以8:2比例划分训练集和测试集\n",
    " \n",
    "train_loader = Data.DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True\n",
    "        )\n",
    "test_loader = Data.DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import LSTM,Module,Linear\n",
    "class MyModel(Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.lstm=LSTM(input_size=11,hidden_size=11,num_layers=2,batch_first=True)\n",
    "        self.linear=Linear(168*11,1*11)#将结果映射到7天的数据\n",
    "    def forward(self,x):\n",
    "        return self.linear(self.lstm(x)[0].reshape(-1,168*11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loaded_model = torch.load('0to167predict168.pkl')\n",
    "\n",
    "model=MyModel()\n",
    "model.load_state_dict(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> EPOCH1 averTrainLoss:0.027 averTestLoss:2.371\n",
      ">>> EPOCH2 averTrainLoss:2.408 averTestLoss:0.149\n",
      ">>> EPOCH3 averTrainLoss:0.148 averTestLoss:0.756\n",
      ">>> EPOCH4 averTrainLoss:0.737 averTestLoss:1.384\n",
      ">>> EPOCH5 averTrainLoss:1.297 averTestLoss:0.903\n",
      ">>> EPOCH6 averTrainLoss:0.925 averTestLoss:0.257\n",
      ">>> EPOCH7 averTrainLoss:0.255 averTestLoss:0.160\n",
      ">>> EPOCH8 averTrainLoss:0.181 averTestLoss:0.435\n",
      ">>> EPOCH9 averTrainLoss:0.467 averTestLoss:0.597\n",
      ">>> EPOCH10 averTrainLoss:0.619 averTestLoss:0.475\n",
      ">>> EPOCH11 averTrainLoss:0.499 averTestLoss:0.254\n",
      ">>> EPOCH12 averTrainLoss:0.277 averTestLoss:0.149\n",
      ">>> EPOCH13 averTrainLoss:0.155 averTestLoss:0.186\n",
      ">>> EPOCH14 averTrainLoss:0.178 averTestLoss:0.267\n",
      ">>> EPOCH15 averTrainLoss:0.244 averTestLoss:0.300\n",
      ">>> EPOCH16 averTrainLoss:0.291 averTestLoss:0.256\n",
      ">>> EPOCH17 averTrainLoss:0.228 averTestLoss:0.170\n",
      ">>> EPOCH18 averTrainLoss:0.163 averTestLoss:0.101\n",
      ">>> EPOCH19 averTrainLoss:0.117 averTestLoss:0.080\n",
      ">>> EPOCH20 averTrainLoss:0.090 averTestLoss:0.102\n",
      ">>> EPOCH21 averTrainLoss:0.106 averTestLoss:0.134\n",
      ">>> EPOCH22 averTrainLoss:0.171 averTestLoss:0.135\n",
      ">>> EPOCH23 averTrainLoss:0.149 averTestLoss:0.104\n",
      ">>> EPOCH24 averTrainLoss:0.112 averTestLoss:0.070\n",
      ">>> EPOCH25 averTrainLoss:0.089 averTestLoss:0.053\n",
      ">>> EPOCH26 averTrainLoss:0.044 averTestLoss:0.067\n",
      ">>> EPOCH27 averTrainLoss:0.083 averTestLoss:0.087\n",
      ">>> EPOCH28 averTrainLoss:0.081 averTestLoss:0.092\n",
      ">>> EPOCH29 averTrainLoss:0.117 averTestLoss:0.067\n",
      ">>> EPOCH30 averTrainLoss:0.069 averTestLoss:0.037\n",
      ">>> EPOCH31 averTrainLoss:0.035 averTestLoss:0.030\n",
      ">>> EPOCH32 averTrainLoss:0.042 averTestLoss:0.048\n",
      ">>> EPOCH33 averTrainLoss:0.047 averTestLoss:0.063\n",
      ">>> EPOCH34 averTrainLoss:0.069 averTestLoss:0.059\n",
      ">>> EPOCH35 averTrainLoss:0.064 averTestLoss:0.039\n",
      ">>> EPOCH36 averTrainLoss:0.047 averTestLoss:0.027\n",
      ">>> EPOCH37 averTrainLoss:0.019 averTestLoss:0.031\n",
      ">>> EPOCH38 averTrainLoss:0.034 averTestLoss:0.041\n",
      ">>> EPOCH39 averTrainLoss:0.054 averTestLoss:0.041\n",
      ">>> EPOCH40 averTrainLoss:0.046 averTestLoss:0.032\n",
      ">>> EPOCH41 averTrainLoss:0.041 averTestLoss:0.025\n",
      ">>> EPOCH42 averTrainLoss:0.020 averTestLoss:0.028\n",
      ">>> EPOCH43 averTrainLoss:0.045 averTestLoss:0.034\n",
      ">>> EPOCH44 averTrainLoss:0.041 averTestLoss:0.035\n",
      ">>> EPOCH45 averTrainLoss:0.032 averTestLoss:0.031\n",
      ">>> EPOCH46 averTrainLoss:0.033 averTestLoss:0.027\n",
      ">>> EPOCH47 averTrainLoss:0.031 averTestLoss:0.025\n",
      ">>> EPOCH48 averTrainLoss:0.029 averTestLoss:0.025\n",
      ">>> EPOCH49 averTrainLoss:0.023 averTestLoss:0.027\n",
      ">>> EPOCH50 averTrainLoss:0.032 averTestLoss:0.028\n",
      ">>> EPOCH51 averTrainLoss:0.038 averTestLoss:0.026\n",
      ">>> EPOCH52 averTrainLoss:0.031 averTestLoss:0.023\n",
      ">>> EPOCH53 averTrainLoss:0.019 averTestLoss:0.022\n",
      ">>> EPOCH54 averTrainLoss:0.024 averTestLoss:0.023\n",
      ">>> EPOCH55 averTrainLoss:0.018 averTestLoss:0.024\n",
      ">>> EPOCH56 averTrainLoss:0.029 averTestLoss:0.023\n",
      ">>> EPOCH57 averTrainLoss:0.023 averTestLoss:0.021\n",
      ">>> EPOCH58 averTrainLoss:0.019 averTestLoss:0.020\n",
      ">>> EPOCH59 averTrainLoss:0.057 averTestLoss:0.022\n",
      ">>> EPOCH60 averTrainLoss:0.028 averTestLoss:0.023\n",
      ">>> EPOCH61 averTrainLoss:0.029 averTestLoss:0.023\n",
      ">>> EPOCH62 averTrainLoss:0.030 averTestLoss:0.021\n",
      ">>> EPOCH63 averTrainLoss:0.026 averTestLoss:0.020\n",
      ">>> EPOCH64 averTrainLoss:0.025 averTestLoss:0.020\n",
      ">>> EPOCH65 averTrainLoss:0.024 averTestLoss:0.021\n",
      ">>> EPOCH66 averTrainLoss:0.021 averTestLoss:0.021\n",
      ">>> EPOCH67 averTrainLoss:0.018 averTestLoss:0.021\n",
      ">>> EPOCH68 averTrainLoss:0.033 averTestLoss:0.020\n",
      ">>> EPOCH69 averTrainLoss:0.030 averTestLoss:0.021\n",
      ">>> EPOCH70 averTrainLoss:0.019 averTestLoss:0.021\n",
      ">>> EPOCH71 averTrainLoss:0.017 averTestLoss:0.020\n",
      ">>> EPOCH72 averTrainLoss:0.021 averTestLoss:0.019\n",
      ">>> EPOCH73 averTrainLoss:0.021 averTestLoss:0.019\n",
      ">>> EPOCH74 averTrainLoss:0.025 averTestLoss:0.019\n",
      ">>> EPOCH75 averTrainLoss:0.030 averTestLoss:0.019\n",
      ">>> EPOCH76 averTrainLoss:0.031 averTestLoss:0.018\n",
      ">>> EPOCH77 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH78 averTrainLoss:0.014 averTestLoss:0.019\n",
      ">>> EPOCH79 averTrainLoss:0.017 averTestLoss:0.020\n",
      ">>> EPOCH80 averTrainLoss:0.016 averTestLoss:0.021\n",
      ">>> EPOCH81 averTrainLoss:0.042 averTestLoss:0.019\n",
      ">>> EPOCH82 averTrainLoss:0.028 averTestLoss:0.018\n",
      ">>> EPOCH83 averTrainLoss:0.028 averTestLoss:0.018\n",
      ">>> EPOCH84 averTrainLoss:0.014 averTestLoss:0.018\n",
      ">>> EPOCH85 averTrainLoss:0.032 averTestLoss:0.018\n",
      ">>> EPOCH86 averTrainLoss:0.018 averTestLoss:0.018\n",
      ">>> EPOCH87 averTrainLoss:0.024 averTestLoss:0.018\n",
      ">>> EPOCH88 averTrainLoss:0.027 averTestLoss:0.019\n",
      ">>> EPOCH89 averTrainLoss:0.024 averTestLoss:0.019\n",
      ">>> EPOCH90 averTrainLoss:0.017 averTestLoss:0.019\n",
      ">>> EPOCH91 averTrainLoss:0.025 averTestLoss:0.018\n",
      ">>> EPOCH92 averTrainLoss:0.032 averTestLoss:0.018\n",
      ">>> EPOCH93 averTrainLoss:0.032 averTestLoss:0.018\n",
      ">>> EPOCH94 averTrainLoss:0.034 averTestLoss:0.018\n",
      ">>> EPOCH95 averTrainLoss:0.026 averTestLoss:0.020\n",
      ">>> EPOCH96 averTrainLoss:0.025 averTestLoss:0.020\n",
      ">>> EPOCH97 averTrainLoss:0.038 averTestLoss:0.020\n",
      ">>> EPOCH98 averTrainLoss:0.021 averTestLoss:0.020\n",
      ">>> EPOCH99 averTrainLoss:0.020 averTestLoss:0.020\n",
      ">>> EPOCH100 averTrainLoss:0.017 averTestLoss:0.019\n",
      ">>> EPOCH101 averTrainLoss:0.017 averTestLoss:0.019\n",
      ">>> EPOCH102 averTrainLoss:0.022 averTestLoss:0.020\n",
      ">>> EPOCH103 averTrainLoss:0.029 averTestLoss:0.021\n",
      ">>> EPOCH104 averTrainLoss:0.029 averTestLoss:0.021\n",
      ">>> EPOCH105 averTrainLoss:0.016 averTestLoss:0.020\n",
      ">>> EPOCH106 averTrainLoss:0.017 averTestLoss:0.021\n",
      ">>> EPOCH107 averTrainLoss:0.027 averTestLoss:0.021\n",
      ">>> EPOCH108 averTrainLoss:0.021 averTestLoss:0.020\n",
      ">>> EPOCH109 averTrainLoss:0.020 averTestLoss:0.020\n",
      ">>> EPOCH110 averTrainLoss:0.022 averTestLoss:0.021\n",
      ">>> EPOCH111 averTrainLoss:0.021 averTestLoss:0.021\n",
      ">>> EPOCH112 averTrainLoss:0.020 averTestLoss:0.020\n",
      ">>> EPOCH113 averTrainLoss:0.036 averTestLoss:0.020\n",
      ">>> EPOCH114 averTrainLoss:0.028 averTestLoss:0.019\n",
      ">>> EPOCH115 averTrainLoss:0.017 averTestLoss:0.018\n",
      ">>> EPOCH116 averTrainLoss:0.024 averTestLoss:0.019\n",
      ">>> EPOCH117 averTrainLoss:0.023 averTestLoss:0.019\n",
      ">>> EPOCH118 averTrainLoss:0.029 averTestLoss:0.020\n",
      ">>> EPOCH119 averTrainLoss:0.031 averTestLoss:0.019\n",
      ">>> EPOCH120 averTrainLoss:0.021 averTestLoss:0.019\n",
      ">>> EPOCH121 averTrainLoss:0.016 averTestLoss:0.019\n",
      ">>> EPOCH122 averTrainLoss:0.032 averTestLoss:0.018\n",
      ">>> EPOCH123 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH124 averTrainLoss:0.027 averTestLoss:0.019\n",
      ">>> EPOCH125 averTrainLoss:0.016 averTestLoss:0.020\n",
      ">>> EPOCH126 averTrainLoss:0.019 averTestLoss:0.018\n",
      ">>> EPOCH127 averTrainLoss:0.023 averTestLoss:0.018\n",
      ">>> EPOCH128 averTrainLoss:0.020 averTestLoss:0.019\n",
      ">>> EPOCH129 averTrainLoss:0.026 averTestLoss:0.019\n",
      ">>> EPOCH130 averTrainLoss:0.043 averTestLoss:0.019\n",
      ">>> EPOCH131 averTrainLoss:0.031 averTestLoss:0.021\n",
      ">>> EPOCH132 averTrainLoss:0.018 averTestLoss:0.022\n",
      ">>> EPOCH133 averTrainLoss:0.025 averTestLoss:0.019\n",
      ">>> EPOCH134 averTrainLoss:0.026 averTestLoss:0.018\n",
      ">>> EPOCH135 averTrainLoss:0.014 averTestLoss:0.019\n",
      ">>> EPOCH136 averTrainLoss:0.026 averTestLoss:0.018\n",
      ">>> EPOCH137 averTrainLoss:0.028 averTestLoss:0.020\n",
      ">>> EPOCH138 averTrainLoss:0.024 averTestLoss:0.021\n",
      ">>> EPOCH139 averTrainLoss:0.020 averTestLoss:0.020\n",
      ">>> EPOCH140 averTrainLoss:0.027 averTestLoss:0.019\n",
      ">>> EPOCH141 averTrainLoss:0.018 averTestLoss:0.018\n",
      ">>> EPOCH142 averTrainLoss:0.033 averTestLoss:0.018\n",
      ">>> EPOCH143 averTrainLoss:0.013 averTestLoss:0.019\n",
      ">>> EPOCH144 averTrainLoss:0.027 averTestLoss:0.020\n",
      ">>> EPOCH145 averTrainLoss:0.040 averTestLoss:0.020\n",
      ">>> EPOCH146 averTrainLoss:0.019 averTestLoss:0.021\n",
      ">>> EPOCH147 averTrainLoss:0.024 averTestLoss:0.020\n",
      ">>> EPOCH148 averTrainLoss:0.022 averTestLoss:0.019\n",
      ">>> EPOCH149 averTrainLoss:0.021 averTestLoss:0.019\n",
      ">>> EPOCH150 averTrainLoss:0.019 averTestLoss:0.020\n",
      ">>> EPOCH151 averTrainLoss:0.016 averTestLoss:0.020\n",
      ">>> EPOCH152 averTrainLoss:0.025 averTestLoss:0.019\n",
      ">>> EPOCH153 averTrainLoss:0.016 averTestLoss:0.020\n",
      ">>> EPOCH154 averTrainLoss:0.019 averTestLoss:0.019\n",
      ">>> EPOCH155 averTrainLoss:0.021 averTestLoss:0.019\n",
      ">>> EPOCH156 averTrainLoss:0.019 averTestLoss:0.019\n",
      ">>> EPOCH157 averTrainLoss:0.020 averTestLoss:0.018\n",
      ">>> EPOCH158 averTrainLoss:0.024 averTestLoss:0.018\n",
      ">>> EPOCH159 averTrainLoss:0.019 averTestLoss:0.019\n",
      ">>> EPOCH160 averTrainLoss:0.025 averTestLoss:0.019\n",
      ">>> EPOCH161 averTrainLoss:0.013 averTestLoss:0.019\n",
      ">>> EPOCH162 averTrainLoss:0.014 averTestLoss:0.019\n",
      ">>> EPOCH163 averTrainLoss:0.029 averTestLoss:0.019\n",
      ">>> EPOCH164 averTrainLoss:0.025 averTestLoss:0.019\n",
      ">>> EPOCH165 averTrainLoss:0.024 averTestLoss:0.019\n",
      ">>> EPOCH166 averTrainLoss:0.018 averTestLoss:0.019\n",
      ">>> EPOCH167 averTrainLoss:0.022 averTestLoss:0.021\n",
      ">>> EPOCH168 averTrainLoss:0.018 averTestLoss:0.022\n",
      ">>> EPOCH169 averTrainLoss:0.030 averTestLoss:0.021\n",
      ">>> EPOCH170 averTrainLoss:0.029 averTestLoss:0.021\n",
      ">>> EPOCH171 averTrainLoss:0.020 averTestLoss:0.021\n",
      ">>> EPOCH172 averTrainLoss:0.017 averTestLoss:0.022\n",
      ">>> EPOCH173 averTrainLoss:0.039 averTestLoss:0.021\n",
      ">>> EPOCH174 averTrainLoss:0.028 averTestLoss:0.020\n",
      ">>> EPOCH175 averTrainLoss:0.025 averTestLoss:0.021\n",
      ">>> EPOCH176 averTrainLoss:0.033 averTestLoss:0.021\n",
      ">>> EPOCH177 averTrainLoss:0.031 averTestLoss:0.020\n",
      ">>> EPOCH178 averTrainLoss:0.019 averTestLoss:0.020\n",
      ">>> EPOCH179 averTrainLoss:0.015 averTestLoss:0.021\n",
      ">>> EPOCH180 averTrainLoss:0.035 averTestLoss:0.022\n",
      ">>> EPOCH181 averTrainLoss:0.027 averTestLoss:0.021\n",
      ">>> EPOCH182 averTrainLoss:0.023 averTestLoss:0.021\n",
      ">>> EPOCH183 averTrainLoss:0.019 averTestLoss:0.021\n",
      ">>> EPOCH184 averTrainLoss:0.025 averTestLoss:0.019\n",
      ">>> EPOCH185 averTrainLoss:0.017 averTestLoss:0.018\n",
      ">>> EPOCH186 averTrainLoss:0.020 averTestLoss:0.018\n",
      ">>> EPOCH187 averTrainLoss:0.017 averTestLoss:0.019\n",
      ">>> EPOCH188 averTrainLoss:0.021 averTestLoss:0.022\n",
      ">>> EPOCH189 averTrainLoss:0.031 averTestLoss:0.021\n",
      ">>> EPOCH190 averTrainLoss:0.032 averTestLoss:0.019\n",
      ">>> EPOCH191 averTrainLoss:0.040 averTestLoss:0.019\n",
      ">>> EPOCH192 averTrainLoss:0.030 averTestLoss:0.019\n",
      ">>> EPOCH193 averTrainLoss:0.017 averTestLoss:0.019\n",
      ">>> EPOCH194 averTrainLoss:0.033 averTestLoss:0.021\n",
      ">>> EPOCH195 averTrainLoss:0.028 averTestLoss:0.025\n",
      ">>> EPOCH196 averTrainLoss:0.032 averTestLoss:0.023\n",
      ">>> EPOCH197 averTrainLoss:0.023 averTestLoss:0.023\n",
      ">>> EPOCH198 averTrainLoss:0.042 averTestLoss:0.024\n",
      ">>> EPOCH199 averTrainLoss:0.087 averTestLoss:0.019\n",
      ">>> EPOCH200 averTrainLoss:0.036 averTestLoss:0.029\n",
      ">>> EPOCH201 averTrainLoss:0.030 averTestLoss:0.026\n",
      ">>> EPOCH202 averTrainLoss:0.050 averTestLoss:0.020\n",
      ">>> EPOCH203 averTrainLoss:0.028 averTestLoss:0.029\n",
      ">>> EPOCH204 averTrainLoss:0.031 averTestLoss:0.024\n",
      ">>> EPOCH205 averTrainLoss:0.030 averTestLoss:0.021\n",
      ">>> EPOCH206 averTrainLoss:0.024 averTestLoss:0.024\n",
      ">>> EPOCH207 averTrainLoss:0.021 averTestLoss:0.022\n",
      ">>> EPOCH208 averTrainLoss:0.020 averTestLoss:0.020\n",
      ">>> EPOCH209 averTrainLoss:0.018 averTestLoss:0.022\n",
      ">>> EPOCH210 averTrainLoss:0.029 averTestLoss:0.020\n",
      ">>> EPOCH211 averTrainLoss:0.016 averTestLoss:0.023\n",
      ">>> EPOCH212 averTrainLoss:0.030 averTestLoss:0.023\n",
      ">>> EPOCH213 averTrainLoss:0.023 averTestLoss:0.020\n",
      ">>> EPOCH214 averTrainLoss:0.028 averTestLoss:0.021\n",
      ">>> EPOCH215 averTrainLoss:0.022 averTestLoss:0.020\n",
      ">>> EPOCH216 averTrainLoss:0.026 averTestLoss:0.020\n",
      ">>> EPOCH217 averTrainLoss:0.028 averTestLoss:0.021\n",
      ">>> EPOCH218 averTrainLoss:0.026 averTestLoss:0.021\n",
      ">>> EPOCH219 averTrainLoss:0.028 averTestLoss:0.021\n",
      ">>> EPOCH220 averTrainLoss:0.026 averTestLoss:0.023\n",
      ">>> EPOCH221 averTrainLoss:0.020 averTestLoss:0.020\n",
      ">>> EPOCH222 averTrainLoss:0.022 averTestLoss:0.019\n",
      ">>> EPOCH223 averTrainLoss:0.027 averTestLoss:0.023\n",
      ">>> EPOCH224 averTrainLoss:0.020 averTestLoss:0.019\n",
      ">>> EPOCH225 averTrainLoss:0.018 averTestLoss:0.019\n",
      ">>> EPOCH226 averTrainLoss:0.012 averTestLoss:0.021\n",
      ">>> EPOCH227 averTrainLoss:0.029 averTestLoss:0.021\n",
      ">>> EPOCH228 averTrainLoss:0.024 averTestLoss:0.022\n",
      ">>> EPOCH229 averTrainLoss:0.017 averTestLoss:0.022\n",
      ">>> EPOCH230 averTrainLoss:0.019 averTestLoss:0.019\n",
      ">>> EPOCH231 averTrainLoss:0.017 averTestLoss:0.018\n",
      ">>> EPOCH232 averTrainLoss:0.023 averTestLoss:0.018\n",
      ">>> EPOCH233 averTrainLoss:0.025 averTestLoss:0.018\n",
      ">>> EPOCH234 averTrainLoss:0.025 averTestLoss:0.021\n",
      ">>> EPOCH235 averTrainLoss:0.023 averTestLoss:0.021\n",
      ">>> EPOCH236 averTrainLoss:0.021 averTestLoss:0.019\n",
      ">>> EPOCH237 averTrainLoss:0.025 averTestLoss:0.019\n",
      ">>> EPOCH238 averTrainLoss:0.017 averTestLoss:0.018\n",
      ">>> EPOCH239 averTrainLoss:0.019 averTestLoss:0.017\n",
      ">>> EPOCH240 averTrainLoss:0.015 averTestLoss:0.017\n",
      ">>> EPOCH241 averTrainLoss:0.027 averTestLoss:0.017\n",
      ">>> EPOCH242 averTrainLoss:0.017 averTestLoss:0.017\n",
      ">>> EPOCH243 averTrainLoss:0.019 averTestLoss:0.019\n",
      ">>> EPOCH244 averTrainLoss:0.034 averTestLoss:0.018\n",
      ">>> EPOCH245 averTrainLoss:0.019 averTestLoss:0.017\n",
      ">>> EPOCH246 averTrainLoss:0.016 averTestLoss:0.020\n",
      ">>> EPOCH247 averTrainLoss:0.020 averTestLoss:0.021\n",
      ">>> EPOCH248 averTrainLoss:0.021 averTestLoss:0.018\n",
      ">>> EPOCH249 averTrainLoss:0.032 averTestLoss:0.017\n",
      ">>> EPOCH250 averTrainLoss:0.019 averTestLoss:0.018\n",
      ">>> EPOCH251 averTrainLoss:0.027 averTestLoss:0.018\n",
      ">>> EPOCH252 averTrainLoss:0.016 averTestLoss:0.020\n",
      ">>> EPOCH253 averTrainLoss:0.019 averTestLoss:0.022\n",
      ">>> EPOCH254 averTrainLoss:0.035 averTestLoss:0.019\n",
      ">>> EPOCH255 averTrainLoss:0.024 averTestLoss:0.018\n",
      ">>> EPOCH256 averTrainLoss:0.018 averTestLoss:0.018\n",
      ">>> EPOCH257 averTrainLoss:0.022 averTestLoss:0.019\n",
      ">>> EPOCH258 averTrainLoss:0.021 averTestLoss:0.019\n",
      ">>> EPOCH259 averTrainLoss:0.023 averTestLoss:0.021\n",
      ">>> EPOCH260 averTrainLoss:0.015 averTestLoss:0.024\n",
      ">>> EPOCH261 averTrainLoss:0.029 averTestLoss:0.018\n",
      ">>> EPOCH262 averTrainLoss:0.023 averTestLoss:0.021\n",
      ">>> EPOCH263 averTrainLoss:0.021 averTestLoss:0.025\n",
      ">>> EPOCH264 averTrainLoss:0.038 averTestLoss:0.018\n",
      ">>> EPOCH265 averTrainLoss:0.024 averTestLoss:0.019\n",
      ">>> EPOCH266 averTrainLoss:0.024 averTestLoss:0.023\n",
      ">>> EPOCH267 averTrainLoss:0.033 averTestLoss:0.017\n",
      ">>> EPOCH268 averTrainLoss:0.028 averTestLoss:0.020\n",
      ">>> EPOCH269 averTrainLoss:0.020 averTestLoss:0.024\n",
      ">>> EPOCH270 averTrainLoss:0.067 averTestLoss:0.018\n",
      ">>> EPOCH271 averTrainLoss:0.025 averTestLoss:0.024\n",
      ">>> EPOCH272 averTrainLoss:0.018 averTestLoss:0.023\n",
      ">>> EPOCH273 averTrainLoss:0.019 averTestLoss:0.022\n",
      ">>> EPOCH274 averTrainLoss:0.031 averTestLoss:0.029\n",
      ">>> EPOCH275 averTrainLoss:0.027 averTestLoss:0.027\n",
      ">>> EPOCH276 averTrainLoss:0.040 averTestLoss:0.024\n",
      ">>> EPOCH277 averTrainLoss:0.019 averTestLoss:0.026\n",
      ">>> EPOCH278 averTrainLoss:0.039 averTestLoss:0.021\n",
      ">>> EPOCH279 averTrainLoss:0.016 averTestLoss:0.022\n",
      ">>> EPOCH280 averTrainLoss:0.054 averTestLoss:0.022\n",
      ">>> EPOCH281 averTrainLoss:0.021 averTestLoss:0.019\n",
      ">>> EPOCH282 averTrainLoss:0.026 averTestLoss:0.020\n",
      ">>> EPOCH283 averTrainLoss:0.030 averTestLoss:0.019\n",
      ">>> EPOCH284 averTrainLoss:0.025 averTestLoss:0.017\n",
      ">>> EPOCH285 averTrainLoss:0.023 averTestLoss:0.020\n",
      ">>> EPOCH286 averTrainLoss:0.023 averTestLoss:0.018\n",
      ">>> EPOCH287 averTrainLoss:0.039 averTestLoss:0.017\n",
      ">>> EPOCH288 averTrainLoss:0.013 averTestLoss:0.018\n",
      ">>> EPOCH289 averTrainLoss:0.027 averTestLoss:0.018\n",
      ">>> EPOCH290 averTrainLoss:0.016 averTestLoss:0.019\n",
      ">>> EPOCH291 averTrainLoss:0.028 averTestLoss:0.018\n",
      ">>> EPOCH292 averTrainLoss:0.029 averTestLoss:0.019\n",
      ">>> EPOCH293 averTrainLoss:0.020 averTestLoss:0.020\n",
      ">>> EPOCH294 averTrainLoss:0.018 averTestLoss:0.018\n",
      ">>> EPOCH295 averTrainLoss:0.015 averTestLoss:0.019\n",
      ">>> EPOCH296 averTrainLoss:0.015 averTestLoss:0.020\n",
      ">>> EPOCH297 averTrainLoss:0.018 averTestLoss:0.019\n",
      ">>> EPOCH298 averTrainLoss:0.026 averTestLoss:0.019\n",
      ">>> EPOCH299 averTrainLoss:0.027 averTestLoss:0.020\n",
      ">>> EPOCH300 averTrainLoss:0.016 averTestLoss:0.018\n",
      ">>> EPOCH301 averTrainLoss:0.015 averTestLoss:0.019\n",
      ">>> EPOCH302 averTrainLoss:0.015 averTestLoss:0.020\n",
      ">>> EPOCH303 averTrainLoss:0.029 averTestLoss:0.020\n",
      ">>> EPOCH304 averTrainLoss:0.021 averTestLoss:0.020\n",
      ">>> EPOCH305 averTrainLoss:0.024 averTestLoss:0.018\n",
      ">>> EPOCH306 averTrainLoss:0.018 averTestLoss:0.017\n",
      ">>> EPOCH307 averTrainLoss:0.021 averTestLoss:0.017\n",
      ">>> EPOCH308 averTrainLoss:0.037 averTestLoss:0.017\n",
      ">>> EPOCH309 averTrainLoss:0.019 averTestLoss:0.017\n",
      ">>> EPOCH310 averTrainLoss:0.024 averTestLoss:0.017\n",
      ">>> EPOCH311 averTrainLoss:0.018 averTestLoss:0.016\n",
      ">>> EPOCH312 averTrainLoss:0.019 averTestLoss:0.017\n",
      ">>> EPOCH313 averTrainLoss:0.015 averTestLoss:0.017\n",
      ">>> EPOCH314 averTrainLoss:0.013 averTestLoss:0.017\n",
      ">>> EPOCH315 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH316 averTrainLoss:0.016 averTestLoss:0.018\n",
      ">>> EPOCH317 averTrainLoss:0.017 averTestLoss:0.018\n",
      ">>> EPOCH318 averTrainLoss:0.020 averTestLoss:0.018\n",
      ">>> EPOCH319 averTrainLoss:0.024 averTestLoss:0.017\n",
      ">>> EPOCH320 averTrainLoss:0.018 averTestLoss:0.016\n",
      ">>> EPOCH321 averTrainLoss:0.017 averTestLoss:0.017\n",
      ">>> EPOCH322 averTrainLoss:0.022 averTestLoss:0.019\n",
      ">>> EPOCH323 averTrainLoss:0.021 averTestLoss:0.017\n",
      ">>> EPOCH324 averTrainLoss:0.015 averTestLoss:0.016\n",
      ">>> EPOCH325 averTrainLoss:0.025 averTestLoss:0.017\n",
      ">>> EPOCH326 averTrainLoss:0.026 averTestLoss:0.016\n",
      ">>> EPOCH327 averTrainLoss:0.013 averTestLoss:0.016\n",
      ">>> EPOCH328 averTrainLoss:0.024 averTestLoss:0.018\n",
      ">>> EPOCH329 averTrainLoss:0.022 averTestLoss:0.020\n",
      ">>> EPOCH330 averTrainLoss:0.030 averTestLoss:0.017\n",
      ">>> EPOCH331 averTrainLoss:0.019 averTestLoss:0.020\n",
      ">>> EPOCH332 averTrainLoss:0.029 averTestLoss:0.019\n",
      ">>> EPOCH333 averTrainLoss:0.012 averTestLoss:0.019\n",
      ">>> EPOCH334 averTrainLoss:0.023 averTestLoss:0.020\n",
      ">>> EPOCH335 averTrainLoss:0.023 averTestLoss:0.018\n",
      ">>> EPOCH336 averTrainLoss:0.019 averTestLoss:0.021\n",
      ">>> EPOCH337 averTrainLoss:0.022 averTestLoss:0.022\n",
      ">>> EPOCH338 averTrainLoss:0.020 averTestLoss:0.021\n",
      ">>> EPOCH339 averTrainLoss:0.025 averTestLoss:0.022\n",
      ">>> EPOCH340 averTrainLoss:0.018 averTestLoss:0.022\n",
      ">>> EPOCH341 averTrainLoss:0.024 averTestLoss:0.020\n",
      ">>> EPOCH342 averTrainLoss:0.022 averTestLoss:0.017\n",
      ">>> EPOCH343 averTrainLoss:0.026 averTestLoss:0.017\n",
      ">>> EPOCH344 averTrainLoss:0.031 averTestLoss:0.021\n",
      ">>> EPOCH345 averTrainLoss:0.034 averTestLoss:0.017\n",
      ">>> EPOCH346 averTrainLoss:0.017 averTestLoss:0.017\n",
      ">>> EPOCH347 averTrainLoss:0.027 averTestLoss:0.016\n",
      ">>> EPOCH348 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH349 averTrainLoss:0.027 averTestLoss:0.017\n",
      ">>> EPOCH350 averTrainLoss:0.027 averTestLoss:0.017\n",
      ">>> EPOCH351 averTrainLoss:0.017 averTestLoss:0.018\n",
      ">>> EPOCH352 averTrainLoss:0.015 averTestLoss:0.017\n",
      ">>> EPOCH353 averTrainLoss:0.019 averTestLoss:0.019\n",
      ">>> EPOCH354 averTrainLoss:0.016 averTestLoss:0.018\n",
      ">>> EPOCH355 averTrainLoss:0.019 averTestLoss:0.017\n",
      ">>> EPOCH356 averTrainLoss:0.019 averTestLoss:0.020\n",
      ">>> EPOCH357 averTrainLoss:0.021 averTestLoss:0.018\n",
      ">>> EPOCH358 averTrainLoss:0.018 averTestLoss:0.020\n",
      ">>> EPOCH359 averTrainLoss:0.018 averTestLoss:0.023\n",
      ">>> EPOCH360 averTrainLoss:0.023 averTestLoss:0.017\n",
      ">>> EPOCH361 averTrainLoss:0.015 averTestLoss:0.020\n",
      ">>> EPOCH362 averTrainLoss:0.025 averTestLoss:0.018\n",
      ">>> EPOCH363 averTrainLoss:0.026 averTestLoss:0.019\n",
      ">>> EPOCH364 averTrainLoss:0.025 averTestLoss:0.023\n",
      ">>> EPOCH365 averTrainLoss:0.032 averTestLoss:0.017\n",
      ">>> EPOCH366 averTrainLoss:0.019 averTestLoss:0.022\n",
      ">>> EPOCH367 averTrainLoss:0.032 averTestLoss:0.017\n",
      ">>> EPOCH368 averTrainLoss:0.019 averTestLoss:0.020\n",
      ">>> EPOCH369 averTrainLoss:0.014 averTestLoss:0.022\n",
      ">>> EPOCH370 averTrainLoss:0.024 averTestLoss:0.016\n",
      ">>> EPOCH371 averTrainLoss:0.020 averTestLoss:0.018\n",
      ">>> EPOCH372 averTrainLoss:0.018 averTestLoss:0.016\n",
      ">>> EPOCH373 averTrainLoss:0.018 averTestLoss:0.020\n",
      ">>> EPOCH374 averTrainLoss:0.023 averTestLoss:0.017\n",
      ">>> EPOCH375 averTrainLoss:0.015 averTestLoss:0.016\n",
      ">>> EPOCH376 averTrainLoss:0.013 averTestLoss:0.017\n",
      ">>> EPOCH377 averTrainLoss:0.020 averTestLoss:0.016\n",
      ">>> EPOCH378 averTrainLoss:0.016 averTestLoss:0.019\n",
      ">>> EPOCH379 averTrainLoss:0.028 averTestLoss:0.018\n",
      ">>> EPOCH380 averTrainLoss:0.013 averTestLoss:0.017\n",
      ">>> EPOCH381 averTrainLoss:0.017 averTestLoss:0.016\n",
      ">>> EPOCH382 averTrainLoss:0.019 averTestLoss:0.016\n",
      ">>> EPOCH383 averTrainLoss:0.026 averTestLoss:0.018\n",
      ">>> EPOCH384 averTrainLoss:0.016 averTestLoss:0.017\n",
      ">>> EPOCH385 averTrainLoss:0.018 averTestLoss:0.019\n",
      ">>> EPOCH386 averTrainLoss:0.020 averTestLoss:0.019\n",
      ">>> EPOCH387 averTrainLoss:0.020 averTestLoss:0.015\n",
      ">>> EPOCH388 averTrainLoss:0.015 averTestLoss:0.020\n",
      ">>> EPOCH389 averTrainLoss:0.024 averTestLoss:0.019\n",
      ">>> EPOCH390 averTrainLoss:0.020 averTestLoss:0.015\n",
      ">>> EPOCH391 averTrainLoss:0.019 averTestLoss:0.022\n",
      ">>> EPOCH392 averTrainLoss:0.027 averTestLoss:0.019\n",
      ">>> EPOCH393 averTrainLoss:0.022 averTestLoss:0.020\n",
      ">>> EPOCH394 averTrainLoss:0.018 averTestLoss:0.021\n",
      ">>> EPOCH395 averTrainLoss:0.016 averTestLoss:0.018\n",
      ">>> EPOCH396 averTrainLoss:0.017 averTestLoss:0.018\n",
      ">>> EPOCH397 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH398 averTrainLoss:0.022 averTestLoss:0.020\n",
      ">>> EPOCH399 averTrainLoss:0.014 averTestLoss:0.018\n",
      ">>> EPOCH400 averTrainLoss:0.020 averTestLoss:0.019\n",
      ">>> EPOCH401 averTrainLoss:0.026 averTestLoss:0.019\n",
      ">>> EPOCH402 averTrainLoss:0.023 averTestLoss:0.015\n",
      ">>> EPOCH403 averTrainLoss:0.019 averTestLoss:0.019\n",
      ">>> EPOCH404 averTrainLoss:0.014 averTestLoss:0.019\n",
      ">>> EPOCH405 averTrainLoss:0.030 averTestLoss:0.017\n",
      ">>> EPOCH406 averTrainLoss:0.016 averTestLoss:0.019\n",
      ">>> EPOCH407 averTrainLoss:0.021 averTestLoss:0.018\n",
      ">>> EPOCH408 averTrainLoss:0.019 averTestLoss:0.016\n",
      ">>> EPOCH409 averTrainLoss:0.020 averTestLoss:0.018\n",
      ">>> EPOCH410 averTrainLoss:0.021 averTestLoss:0.017\n",
      ">>> EPOCH411 averTrainLoss:0.020 averTestLoss:0.017\n",
      ">>> EPOCH412 averTrainLoss:0.020 averTestLoss:0.018\n",
      ">>> EPOCH413 averTrainLoss:0.014 averTestLoss:0.017\n",
      ">>> EPOCH414 averTrainLoss:0.022 averTestLoss:0.018\n",
      ">>> EPOCH415 averTrainLoss:0.021 averTestLoss:0.022\n",
      ">>> EPOCH416 averTrainLoss:0.022 averTestLoss:0.018\n",
      ">>> EPOCH417 averTrainLoss:0.013 averTestLoss:0.019\n",
      ">>> EPOCH418 averTrainLoss:0.020 averTestLoss:0.017\n",
      ">>> EPOCH419 averTrainLoss:0.016 averTestLoss:0.018\n",
      ">>> EPOCH420 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH421 averTrainLoss:0.016 averTestLoss:0.017\n",
      ">>> EPOCH422 averTrainLoss:0.024 averTestLoss:0.018\n",
      ">>> EPOCH423 averTrainLoss:0.024 averTestLoss:0.017\n",
      ">>> EPOCH424 averTrainLoss:0.018 averTestLoss:0.016\n",
      ">>> EPOCH425 averTrainLoss:0.017 averTestLoss:0.018\n",
      ">>> EPOCH426 averTrainLoss:0.020 averTestLoss:0.017\n",
      ">>> EPOCH427 averTrainLoss:0.014 averTestLoss:0.017\n",
      ">>> EPOCH428 averTrainLoss:0.013 averTestLoss:0.017\n",
      ">>> EPOCH429 averTrainLoss:0.018 averTestLoss:0.018\n",
      ">>> EPOCH430 averTrainLoss:0.015 averTestLoss:0.016\n",
      ">>> EPOCH431 averTrainLoss:0.011 averTestLoss:0.019\n",
      ">>> EPOCH432 averTrainLoss:0.018 averTestLoss:0.017\n",
      ">>> EPOCH433 averTrainLoss:0.019 averTestLoss:0.020\n",
      ">>> EPOCH434 averTrainLoss:0.031 averTestLoss:0.019\n",
      ">>> EPOCH435 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH436 averTrainLoss:0.017 averTestLoss:0.016\n",
      ">>> EPOCH437 averTrainLoss:0.022 averTestLoss:0.018\n",
      ">>> EPOCH438 averTrainLoss:0.015 averTestLoss:0.019\n",
      ">>> EPOCH439 averTrainLoss:0.022 averTestLoss:0.018\n",
      ">>> EPOCH440 averTrainLoss:0.014 averTestLoss:0.019\n",
      ">>> EPOCH441 averTrainLoss:0.022 averTestLoss:0.022\n",
      ">>> EPOCH442 averTrainLoss:0.019 averTestLoss:0.016\n",
      ">>> EPOCH443 averTrainLoss:0.016 averTestLoss:0.016\n",
      ">>> EPOCH444 averTrainLoss:0.033 averTestLoss:0.016\n",
      ">>> EPOCH445 averTrainLoss:0.009 averTestLoss:0.020\n",
      ">>> EPOCH446 averTrainLoss:0.020 averTestLoss:0.017\n",
      ">>> EPOCH447 averTrainLoss:0.019 averTestLoss:0.016\n",
      ">>> EPOCH448 averTrainLoss:0.019 averTestLoss:0.015\n",
      ">>> EPOCH449 averTrainLoss:0.022 averTestLoss:0.014\n",
      ">>> EPOCH450 averTrainLoss:0.027 averTestLoss:0.016\n",
      ">>> EPOCH451 averTrainLoss:0.016 averTestLoss:0.016\n",
      ">>> EPOCH452 averTrainLoss:0.014 averTestLoss:0.016\n",
      ">>> EPOCH453 averTrainLoss:0.030 averTestLoss:0.015\n",
      ">>> EPOCH454 averTrainLoss:0.016 averTestLoss:0.015\n",
      ">>> EPOCH455 averTrainLoss:0.023 averTestLoss:0.015\n",
      ">>> EPOCH456 averTrainLoss:0.014 averTestLoss:0.014\n",
      ">>> EPOCH457 averTrainLoss:0.014 averTestLoss:0.015\n",
      ">>> EPOCH458 averTrainLoss:0.014 averTestLoss:0.015\n",
      ">>> EPOCH459 averTrainLoss:0.038 averTestLoss:0.014\n",
      ">>> EPOCH460 averTrainLoss:0.019 averTestLoss:0.016\n",
      ">>> EPOCH461 averTrainLoss:0.013 averTestLoss:0.016\n",
      ">>> EPOCH462 averTrainLoss:0.014 averTestLoss:0.015\n",
      ">>> EPOCH463 averTrainLoss:0.016 averTestLoss:0.015\n",
      ">>> EPOCH464 averTrainLoss:0.019 averTestLoss:0.016\n",
      ">>> EPOCH465 averTrainLoss:0.031 averTestLoss:0.017\n",
      ">>> EPOCH466 averTrainLoss:0.020 averTestLoss:0.017\n",
      ">>> EPOCH467 averTrainLoss:0.012 averTestLoss:0.018\n",
      ">>> EPOCH468 averTrainLoss:0.021 averTestLoss:0.016\n",
      ">>> EPOCH469 averTrainLoss:0.020 averTestLoss:0.016\n",
      ">>> EPOCH470 averTrainLoss:0.018 averTestLoss:0.018\n",
      ">>> EPOCH471 averTrainLoss:0.025 averTestLoss:0.018\n",
      ">>> EPOCH472 averTrainLoss:0.015 averTestLoss:0.016\n",
      ">>> EPOCH473 averTrainLoss:0.013 averTestLoss:0.015\n",
      ">>> EPOCH474 averTrainLoss:0.018 averTestLoss:0.016\n",
      ">>> EPOCH475 averTrainLoss:0.018 averTestLoss:0.016\n",
      ">>> EPOCH476 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH477 averTrainLoss:0.020 averTestLoss:0.016\n",
      ">>> EPOCH478 averTrainLoss:0.023 averTestLoss:0.016\n",
      ">>> EPOCH479 averTrainLoss:0.016 averTestLoss:0.017\n",
      ">>> EPOCH480 averTrainLoss:0.022 averTestLoss:0.017\n",
      ">>> EPOCH481 averTrainLoss:0.017 averTestLoss:0.016\n",
      ">>> EPOCH482 averTrainLoss:0.029 averTestLoss:0.017\n",
      ">>> EPOCH483 averTrainLoss:0.023 averTestLoss:0.016\n",
      ">>> EPOCH484 averTrainLoss:0.018 averTestLoss:0.017\n",
      ">>> EPOCH485 averTrainLoss:0.015 averTestLoss:0.017\n",
      ">>> EPOCH486 averTrainLoss:0.011 averTestLoss:0.016\n",
      ">>> EPOCH487 averTrainLoss:0.019 averTestLoss:0.017\n",
      ">>> EPOCH488 averTrainLoss:0.015 averTestLoss:0.017\n",
      ">>> EPOCH489 averTrainLoss:0.020 averTestLoss:0.018\n",
      ">>> EPOCH490 averTrainLoss:0.023 averTestLoss:0.017\n",
      ">>> EPOCH491 averTrainLoss:0.020 averTestLoss:0.017\n",
      ">>> EPOCH492 averTrainLoss:0.016 averTestLoss:0.017\n",
      ">>> EPOCH493 averTrainLoss:0.015 averTestLoss:0.017\n",
      ">>> EPOCH494 averTrainLoss:0.016 averTestLoss:0.018\n",
      ">>> EPOCH495 averTrainLoss:0.014 averTestLoss:0.017\n",
      ">>> EPOCH496 averTrainLoss:0.018 averTestLoss:0.016\n",
      ">>> EPOCH497 averTrainLoss:0.014 averTestLoss:0.017\n",
      ">>> EPOCH498 averTrainLoss:0.017 averTestLoss:0.016\n",
      ">>> EPOCH499 averTrainLoss:0.015 averTestLoss:0.018\n",
      ">>> EPOCH500 averTrainLoss:0.026 averTestLoss:0.018\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lossList=[]#记录训练loss\n",
    "lossListTest=[]#记录测试loss\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    loss_nowEpoch=[]\n",
    "    model.train()\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        out=model(batch_x)#模型输入\n",
    "        Loss = F.mse_loss(out,batch_y.view(-1,11))#loss计算，将batch_y从(64,1,11)变形为(64,11)\n",
    "        optimizer.zero_grad()#当前batch的梯度不会再用到，所以清除梯度\n",
    "        Loss.backward()#反向传播计算梯度\n",
    "        optimizer.step()#更新参数\n",
    "        loss_nowEpoch.append(Loss.item())\n",
    "        break\n",
    "    lossList.append(sum(loss_nowEpoch)/len(loss_nowEpoch))\n",
    " \n",
    "    loss_nowEpochTest = []\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y) in enumerate(test_loader):\n",
    "        out = model(batch_x)\n",
    "        Loss = F.mse_loss(out, batch_y.view(-1, 11))  # 将batch_y从(64,1,11)变形为(64,11)\n",
    "        loss_nowEpochTest.append(Loss.item())\n",
    "        break\n",
    "    lossListTest.append(sum(loss_nowEpochTest)/len(loss_nowEpochTest))\n",
    " \n",
    "    print(\">>> EPOCH{} averTrainLoss:{:.3f} averTestLoss:{:.3f}\".format(epoch+1, lossList[-1],lossListTest[-1]))\n",
    "\n",
    "torch.save(model.state_dict(), '0to167predict168.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(epochs)),lossList,label='Train')\n",
    "plt.plot(list(range(epochs)),lossListTest,label='Test')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX1=[]#属性\n",
    "dataY1=[]#标签\n",
    "k=0\n",
    "tempX=[]#储存某个历史167天数据\n",
    "tempY=[]#储存某个未来1天数据,即第168天\n",
    "for index, rows in test1.iterrows():\n",
    "    if k<168:\n",
    "        k+=1\n",
    "        tempX.append([rows['pay_num'],rows['payment_orders'],rows['refund_orders'],rows['cancel_orders'],rows['income'],rows['refund_income'],rows['actual_orders'],rows['续费率'],rows['实际收入'],rows['保留率'],rows['ltv']])\n",
    "        \n",
    "        continue\n",
    "    if k<169:\n",
    "        k = 0\n",
    "        # print(rows['week'])\n",
    "        tempY.append([rows['pay_num'],rows['payment_orders'],rows['refund_orders'],rows['cancel_orders'],rows['income'],rows['refund_income'],rows['actual_orders'],rows['续费率'],rows['实际收入'],rows['保留率'],rows['ltv']])\n",
    "        \n",
    "        dataX1.append(tempX)\n",
    "        dataY1.append(tempY)\n",
    "        tempX=[]#储存某个历史167天数据\n",
    "        tempY=[]#储存某个未来1天数据,即第168天\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.tensor(dataX1)\n",
    "Y=model(X.view(len(dataX1),168,11).float()).reshape(len(dataX1),11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = []\n",
    "a2 = []\n",
    "for i in Y.tolist():\n",
    "    a1.append(i[-1])\n",
    "\n",
    "for i in dataY1:\n",
    "    a2.append(i[0][-1])\n",
    "\n",
    "a2 = np.array(a2)\n",
    "a1 = np.array(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大于0.1的数量：5\n",
      "小于等于0.1的数量：624\n",
      "合格率： 0.9920508744038156\n"
     ]
    }
   ],
   "source": [
    "# 计算相对误差\n",
    "relative_errors = np.abs((a2 - a1) / a2)\n",
    "\n",
    "# 统计relative_errors中大于0.1的数量\n",
    "count_greater_than_0_1 = (relative_errors > 0.1).sum()\n",
    "\n",
    "# 统计relative_errors中小于等于0.1的数量\n",
    "count_less_than_or_equal_to_0_1 = (relative_errors <= 0.1).sum()\n",
    "\n",
    "print(f\"大于0.1的数量：{count_greater_than_0_1}\")\n",
    "print(f\"小于等于0.1的数量：{count_less_than_or_equal_to_0_1}\")\n",
    "print('合格率：', count_less_than_or_equal_to_0_1/(count_less_than_or_equal_to_0_1+count_greater_than_0_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
